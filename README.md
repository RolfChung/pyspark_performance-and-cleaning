# Summmary



<p>
Data sets consistings of millions and more observations are exceeding the capacities of a single machine. Based on the Hadoop distributed file system Apache Spark offers a solution for processing large, big data sets by building a virtual machine managing file chunks on distributed worker nodes. The goal here is to apply methods for preparing, cleaning and optimizing data sets. This support a smooth execution with good performance.
</p>

<p>
Some topics and methods explored in this context here are:
</p> 

<ul>
  <li>data frames</li>
  <li>SQL</li>
  <li>conditional queries</li>
  <li>Parquet files</li>
  <li>filtering</li>
  <li>pipelines</li>
  <li>caching</li>
  <li>joining</li>
  <li>shuffling</li>
  <li>UDFs</li>
  <li>indexing</li>
</ul> 

<p>
There are two data sets used here: the Dallas voting decisions data set consisting of string types, and the flight system data sets also with integer sets.
</p> 