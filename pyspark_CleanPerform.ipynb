{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summmary\n",
    "\n",
    "\n",
    "\n",
    "<p>\n",
    "Data sets consistings of millions and more observations are exceeding the capacities of a single machine. Based on the Hadoop distributed file system Apache Spark offers a solution for processing large, big data sets by building a virtual machine managing file chunks on distributed worker nodes. The goal here is to apply methods for preparing, cleaning and optimizing data sets. This support a smooth execution with good performance.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Some topics and methods explored in this context here are:\n",
    "</p> \n",
    "\n",
    "<ul>\n",
    "  <li>data frames</li>\n",
    "  <li>SQL</li>\n",
    "  <li>conditional queries</li>\n",
    "  <li>Parquet files</li>\n",
    "  <li>filtering</li>\n",
    "  <li>pipelines</li>\n",
    "  <li>caching</li>\n",
    "  <li>joining</li>\n",
    "  <li>shuffling</li>\n",
    "  <li>UDFs</li>\n",
    "  <li>indexing</li>\n",
    "</ul> \n",
    "\n",
    "<p>\n",
    "There are two data sets used here: the Dallas voting decisions data set consisting of string types, and the flight system data sets also with integer.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INSTALLED VERSIONS\n",
      "------------------\n",
      "commit           : None\n",
      "python           : 3.7.4.final.0\n",
      "python-bits      : 64\n",
      "OS               : Windows\n",
      "OS-release       : 10\n",
      "machine          : AMD64\n",
      "processor        : Intel64 Family 6 Model 58 Stepping 9, GenuineIntel\n",
      "byteorder        : little\n",
      "LC_ALL           : None\n",
      "LANG             : None\n",
      "LOCALE           : None.None\n",
      "\n",
      "pandas           : 0.25.1\n",
      "numpy            : 1.16.5\n",
      "pytz             : 2019.3\n",
      "dateutil         : 2.8.0\n",
      "pip              : 19.2.3\n",
      "setuptools       : 41.4.0\n",
      "Cython           : 0.29.13\n",
      "pytest           : 5.2.1\n",
      "hypothesis       : None\n",
      "sphinx           : 2.2.0\n",
      "blosc            : None\n",
      "feather          : None\n",
      "xlsxwriter       : 1.2.1\n",
      "lxml.etree       : 4.4.1\n",
      "html5lib         : 1.0.1\n",
      "pymysql          : None\n",
      "psycopg2         : None\n",
      "jinja2           : 2.10.3\n",
      "IPython          : 7.8.0\n",
      "pandas_datareader: None\n",
      "bs4              : 4.8.0\n",
      "bottleneck       : 1.2.1\n",
      "fastparquet      : None\n",
      "gcsfs            : None\n",
      "lxml.etree       : 4.4.1\n",
      "matplotlib       : 3.1.1\n",
      "numexpr          : 2.7.0\n",
      "odfpy            : None\n",
      "openpyxl         : 3.0.0\n",
      "pandas_gbq       : None\n",
      "pyarrow          : 0.15.1\n",
      "pytables         : None\n",
      "s3fs             : None\n",
      "scipy            : 1.3.1\n",
      "sqlalchemy       : 1.3.9\n",
      "tables           : 3.5.2\n",
      "xarray           : None\n",
      "xlrd             : 1.2.0\n",
      "xlwt             : 1.3.0\n",
      "xlsxwriter       : 1.2.1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# IPython\n",
    "from IPython.display import display, HTML, display_html \n",
    "#usefull to display wide tables\n",
    "\n",
    "# base packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pathlib\n",
    "import inspect\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# pyspark\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.rdd import RDD\n",
    "# from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, \\\n",
    "StringType, BooleanType, DateType, FloatType\n",
    "from pyspark.sql.functions import lower, col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql.functions import mean, stddev , col, avg, round\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "\n",
    "# machine learning in spark has got different libraries\n",
    "# for different data objects\n",
    "\n",
    "# machine learning on dataframes\n",
    "# is based on pyspark.ml\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# machine learning on RDDs\n",
    "# is based pyspark.mllib\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "# pyspark.mllib.classification.SVMModel\n",
    "from pyspark.mllib.classification import SVMModel, SVMWithSGD\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Collaborative Filtering - RDD-based API\n",
    "# https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark_dist_explore import Histogram, hist, distplot, \\\n",
    "pandas_histogram\n",
    "\n",
    "from handyspark import *\n",
    "\n",
    "print(pd.show_versions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "# print(cwd)\n",
    "\n",
    "cwd_path = pathlib.Path.cwd()\n",
    "# print(cwd_path)\n",
    "\n",
    "home_path = pathlib.Path.home()\n",
    "# print(home_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.0.0-preview\n",
      "Python version: 3.7\n",
      "Spark master: local[*]\n",
      "Application name: PySparkShell\n",
      "Parllelism: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark version: {}\".format(sc.version))\n",
    "print(\"Python version: {}\".format(sc.pythonVer))\n",
    "print(\"Spark master: {}\".format(sc.master))\n",
    "# print(\"Spark user: {}\".format(sc.sparkUser))\n",
    "print(\"Application name: {}\".format(sc.appName))\n",
    "# print(\"Application id: {}\".format(sc.applicationId))\n",
    "print(\"Parllelism: {}\".format(sc.defaultParallelism))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark session\n",
    "\n",
    "<p>\n",
    "<b>Spark Configuration</b><br>\n",
    "from pyspark.sql import SparkSession<br>\n",
    "spark = SparkSession.builder<br>\n",
    ".appName(\"Python Spark regression example\")<br>\n",
    ".config(\"config.option\", \"value\").getOrCreate()<br>\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001D53F4DE588>\n",
      "default\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# this is for example needed to activate the SQL-api\n",
    "spark_session = SparkSession.builder.appName(\"spam_classification\").\\\n",
    "getOrCreate()\n",
    "print(spark_session)\n",
    "print(spark_session.catalog.currentDatabase())\n",
    "print(spark_session.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.0.0-preview\n",
      "Python version: 3.7\n",
      "Spark master: local[*]\n",
      "Application name: PySparkShell\n",
      "Parllelism: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark version: {}\".format(sc.version))\n",
    "print(\"Python version: {}\".format(sc.pythonVer))\n",
    "print(\"Spark master: {}\".format(sc.master))\n",
    "# print(\"Spark user: {}\".format(sc.sparkUser))\n",
    "print(\"Application name: {}\".format(sc.appName))\n",
    "# print(\"Application id: {}\".format(sc.applicationId))\n",
    "print(\"Parllelism: {}\".format(sc.defaultParallelism))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark session\n",
    "\n",
    "<p>\n",
    "<b>Spark Configuration</b><br>\n",
    "from pyspark.sql import SparkSession<br>\n",
    "spark = SparkSession.builder<br>\n",
    ".appName(\"Python Spark regression example\")<br>\n",
    ".config(\"config.option\", \"value\").getOrCreate()<br>\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001D53F4DE588>\n",
      "default\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# this is for example needed to activate the SQL-api\n",
    "spark_session = SparkSession.builder.appName(\"spam_classification\").\\\n",
    "getOrCreate()\n",
    "print(spark_session)\n",
    "print(spark_session.catalog.currentDatabase())\n",
    "print(spark_session.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data: voting decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingDecisions = \\\n",
    "spark.read.csv(path='data_sets\\DallasCouncilVoters.csv',\n",
    "               sep=',', encoding='UTF-8', header=True, inferSchema=True\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a new schema to an existing data frame works by doing the detour over creating a new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- VOTER_NAME: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vdSchema = StructType([\n",
    "    # Define the date field\n",
    "    StructField('DATE', DateType(), True),\n",
    "    # Add the title field\n",
    "    StructField('TITLE', StringType(), True),\n",
    "    # Add the voter field\n",
    "    StructField('VOTER_NAME', StringType(), True) \n",
    "    ])   \n",
    "                            \n",
    "                                \n",
    "votingDecisions_newSchema = spark.createDataFrame(votingDecisions.rdd, schema=vdSchema)\n",
    "votingDecisions_newSchema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- VOTER_NAME: string (nullable = true)\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "n observations: 44625\n",
      "n columns: 3\n"
     ]
    }
   ],
   "source": [
    "votingDecisions.printSchema()\n",
    "print(type(votingDecisions))\n",
    "print(\"n observations: {}\".format(votingDecisions.count()))\n",
    "print(\"n columns: {}\".format(len(votingDecisions.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(votingDecisions.describe().show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null-values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain specific data frame language works only on columns\n",
    "vdTitle_isnull = votingDecisions.filter(votingDecisions.TITLE.isNull())\n",
    "print(type(vdTitle_isnull))\n",
    "print(vdTitle_isnull.count())\n",
    "print(vdTitle_isnull.show(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdVN_isnull = votingDecisions.filter(votingDecisions.VOTER_NAME.isNull())\n",
    "print(type(vdVN_isnull))\n",
    "print(vdVN_isnull.count())\n",
    "print(vdVN_isnull.show(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negating isnull for two columns\n",
    "vd_notNull = votingDecisions.filter(~F.col('TITLE').isNull() | \n",
    "                                     F.col('VOTER_NAME').isNull())\n",
    "\n",
    "print(vd_notNull.count())\n",
    "print(vd_notNull.show(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handyspark offers a convenient way to determine nulls\n",
    "votingDecisions_hdf = votingDecisions.toHandy()\n",
    "print(votingDecisions_hdf.isnull())\n",
    "print(votingDecisions_hdf.isnull(ratio=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingDecisions_hdf_2 = votingDecisions_hdf.na.drop()\n",
    "print(votingDecisions_hdf_2.isnull(ratio=True))\n",
    "print(type(votingDecisions_hdf_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_title = votingDecisions_hdf_2.cols['TITLE'].value_counts()\n",
    "print(type(vc_title))\n",
    "print(vc_title[5:])\n",
    "\n",
    "# displaying the full value counts shows that there values \n",
    "# which are not names\n",
    "# for example there is 15 the string value:\n",
    "# authorize   an   ordinance   approving   and   adopting ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_title[5:].sort_values().plot.bar(figsize=(20,6), edgecolor=\"black\", linewidth=5,\n",
    "                                    color=['darkorange', 'lime'],\n",
    "                                    title='Frequency of titles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_vn = votingDecisions_hdf_2.cols['VOTER_NAME'].value_counts()\n",
    "print(type(vc_vn))\n",
    "print(vc_vn[8:])\n",
    "\n",
    "# displaying the full value counts shows that there values \n",
    "# which are not names\n",
    "# for example there is 15 the string value:\n",
    "# the  final  2018 Assessment Plan and the 2018 Assessment ...\n",
    "# also some index values are not unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_vn[8:].sort_values(ascending=False).plot.bar(figsize=(20,5), \n",
    "                                                edgecolor='black', \n",
    "                                                linewidth=4, color=['purple', 'cyan', 'olive'],\n",
    "                                                title=\"counts of voters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a handy frame back to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting handy frame back to data frame\n",
    "# https://dvgodoy.github.io/handyspark/_modules/handyspark/sql/dataframe.html\n",
    "\n",
    "vd_df = votingDecisions_hdf_2.notHandy()\n",
    "print(type(vd_df))\n",
    "vd_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by converting to pandas\n",
    "vD_pd =votingDecisions_hdf_2.toPandas()\n",
    "vD_pd.columns = ['date', 'title', 'voter']\n",
    "print(vD_pd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by using domain specific pyspark\n",
    "vd_df = vd_df.selectExpr('DATE as date', \n",
    "                           'TITLE as title' , \n",
    "                           'VOTER_NAME as voter')\n",
    "vd_df.show(5)\n",
    "\n",
    "# using the pandas object of HandySpark does not work with rename\n",
    "# votingDecisions_hdf_2.pandas.rename(columns ={'DATE':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumnRenamed('age', 'age2').collect()\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=selectexpr#pyspark.sql.DataFrame.withColumnRenamed\n",
    "\n",
    "vd_cr = votingDecisions.withColumnRenamed('DATE', 'date').\\\n",
    "withColumnRenamed('TITLE', 'title').withColumnRenamed('VOTER_NAME', 'voter')\n",
    "vd_cr.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on data frames\n",
    "\n",
    "with the domain specific Pyspark language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct VOTER_NAME \n",
    "name_dist = vd_df.select('voter').distinct()\n",
    "print(name_dist.count())\n",
    "print(name_dist.show(10, truncate=False))\n",
    "# 35 distinc voter names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_disc = vd_df.select('title').distinct()\n",
    "print(title_disc.count())\n",
    "display(title_disc.show())\n",
    "# 2020]\"| or authorize an  or... ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter voter_df where the VOTER_NAME is 1-15 characters in length\n",
    "\n",
    "vn_len = \\\n",
    "vd_df.filter(\"length(voter) > 1 and length(voter) < 20\")\n",
    "\n",
    "print(type(vn_len))\n",
    "print(vd_df.count())\n",
    "print(vd_df.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ negates filter\n",
    "# filter out voter names with a point\n",
    "vd_point = vd_df.filter(~ F.col('voter').contains('.') )\n",
    "\n",
    "print(type(vd_point))\n",
    "print(vd_point.count())\n",
    "print(vd_point.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_underscore = vd_df.filter(~F.col('voter').contains('_'))\n",
    "print(vd_underscore.count())\n",
    "print(vd_underscore.show(4))\n",
    "\n",
    "vd_with_underscore = vd_df.filter(F.col('voter').contains('_'))\n",
    "print(vd_with_underscore.count())\n",
    "print(vd_with_underscore.show(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_mayors = vd_df.filter(F.col('title') == 'Mayor')\n",
    "\n",
    "print(vd_mayors.count())\n",
    "print(vd_mayors.show(4))\n",
    "\n",
    "vd_mayors_dist = vd_mayors.select('voter').distinct()\n",
    "\n",
    "print(vd_mayors_dist.count())\n",
    "print(vd_mayors_dist.show(5))\n",
    "# Only one mayor: Michael S. Rawlings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_cm = vd_df.filter(F.col('title')=='Councilmember')\n",
    "\n",
    "print(vd_cm.count())\n",
    "print(vd_cm.show(4))\n",
    "\n",
    "vd_cm_dist = vd_cm.select('voter').distinct()\n",
    "print(vd_cm_dist.count())\n",
    "print(vd_cm_dist.show(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new columns\n",
    "\n",
    "<p>\n",
    "<b>withColumn(colName, col)</b><br>\n",
    "Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "</p> \n",
    "\n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=selectexpr#pyspark.sql.DataFrame.withColumnRenamed\" target=\"_blank\">spark.apache.org</a> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = vd_df.withColumn('splits', F.split(vd_df.voter, '\\s+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whitespace = \\s+\n",
    "vd_df_2 = \\\n",
    "vd_df.withColumn('separate_voter', F.split(vd_df.voter, '\\s+'))\n",
    "print(vd_df_2.columns)\n",
    "print(vd_df_2.select('voter', 'separate_voter').show(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_df_2 = \\\n",
    "vd_df_2.withColumn('forename', vd_df_2.separate_voter.getItem(0))\n",
    "\n",
    "print(type(vd_df_2))\n",
    "print(vd_df_2.columns)\n",
    "print(len(vd_df_2.columns))\n",
    "print(vd_df_2.select('title', 'voter', 'separate_voter').show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_df_2 = \\\n",
    "vd_df_2.withColumn('middlename', vd_df_2.separate_voter.getItem(1))\n",
    "\n",
    "print(vd_df_2.select('title', 'voter', 'separate_voter').show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_df_2 = \\\n",
    "vd_df_2.withColumn('surname', vd_df_2.separate_voter.getItem(2))\n",
    "\n",
    "print(vd_df_2.select('title', 'voter', 'surname', 'separate_voter').show(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_df_3 = vd_df_2.drop('separate_voter')\n",
    "vd_df_3.select('title', 'voter', 'surname').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a columns based on conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a simple approach to assign a number to the title \n",
    "vd_df_4 = \\\n",
    "vd_df_3.withColumn(\"encoded\" , F.when(vd_df_3.title=='Councilmember', \n",
    "                                      F.rand()))\n",
    "vd_df_4.select('title', 'voter', 'surname', 'encoded').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_df_4 = vd_df_4.withColumn(\"encoded\",\n",
    "                             when(vd_df_4.title=='Councilmember', 3)\n",
    "                            .when(vd_df_4.title=='Mayor', 1)\n",
    "                            .when(vd_df_4.title=='Deputy Mayor Pro Tem',2)\n",
    "                            .when(vd_df_4.title=='Mayor Pro Tem',2 )\n",
    "                            .otherwise(0))\n",
    "\n",
    "print(type(vd_df_4))\n",
    "print(vd_df_4.count())\n",
    "print(vd_df_4.select('title', 'voter', 'surname', 'encoded').show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet\n",
    "\n",
    "<p>\n",
    "CSV files are low to parse and requires constantly redefining the schema. \n",
    "An alternative in Apache Spark and other data processing frameworks \n",
    "is the columnar data format \"Parquet\".\n",
    "It limits input-output operations, stores automatically schemas, needs less space, and \n",
    "makes access to certain columns easy.\n",
    "</p> \n",
    "\n",
    "<p>\n",
    "Parquet speeds up operations by applying the technique of \"predicate pushdown\"\n",
    "processing only the data needed for the operation and not the entire data frame.\n",
    "</p> \n",
    "\n",
    "<a href=\"https://www.tutorialspoint.com/spark_sql/spark_sql_parquet_files.htm\" target=\"_blank\">\n",
    "Spark SQL - Parquet Files</a> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying sampling, not really necessary here\n",
    "vd_sample_1 = vd_df_4.sample(withReplacement=False, fraction=0.5, seed=43)\n",
    "vd_sample_2 = vd_df_4.sample(withReplacement=False, fraction=0.5, seed=43)\n",
    "print(vd_sample_1.count())\n",
    "print(vd_sample_2.count())\n",
    "print(vd_df_4.count())\n",
    "print(22101 + 22101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_union = vd_sample_1.union(vd_sample_2)\n",
    "print(vd_union.count())\n",
    "vd_union.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the data frame into the Parquet-format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_union.write.parquet(\"data_sets/vd_parquet_1.parquet\", mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the parquet into a new data frame. After this the data frame is ready for doing operations like \"count\" or others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(spark.read.parquet('data_sets/vd_parquet_1.parquet')))\n",
    "print(spark.read.parquet('data_sets/vd_parquet_1.parquet').count())\n",
    "print(spark.read.parquet('data_sets/vd_parquet_1.parquet').printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL-queries on parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_union.write.parquet(\"data_sets/vd_parquet_2.parquet\", mode='overwrite')\n",
    "vd_parquet_file = spark.read.parquet('data_sets/vd_parquet_2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Parquet files can also be used to create a temporary view and then used in SQL statements,\n",
    "maintaining all the advantages of a parquet file named above.\n",
    "</p> \n",
    "<a href=\"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\" target=\"_blank\">Parquet Files</a> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_parquet_file.createOrReplaceTempView(\"vd_parquetFile\")\n",
    "print(spark_session.catalog.listTables())\n",
    "# the parquet file is now registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql1 = spark.sql(\"SELECT title FROM vd_parquetFile LIMIT 10\")\n",
    "sql1.show()\n",
    "print(sql1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql2 = spark.sql(\"SELECT title surname FROM vd_parquetFile WHERE title == 'Mayor'\")\n",
    "print(sql2.show(5))\n",
    "print(sql2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql3 = spark.sql(\"SELECT DISTINCT surname  FROM vd_parquetFile WHERE title LIKE 'Mayor'\")\n",
    "print(sql3.show())\n",
    "print(sql3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql4 = spark.sql(\"SELECT title, surname FROM vd_parquetFile WHERE title IN ('Mayor', 'Councilmember')\")\n",
    "print(sql4.show(5))\n",
    "print(sql4.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql5 = \\\n",
    "spark.sql(\"SELECT title, COUNT(title) FROM vd_parquetFile WHERE title IN \\\n",
    "('Mayor', 'Councilmember') GROUP BY title\")\n",
    "print(sql5.show(5))\n",
    "print(sql5.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section new data setd are introduced containing also variables \n",
    "of type integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data: airplane departures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading multiple files with pandas and glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "\n",
    "departures_files = sorted(glob('data_sets\\AA_DFW_*.csv'))\n",
    "print(type(departures_files  ))\n",
    "print(len(departures_files  ))\n",
    "print(departures_files )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dep = pd.concat((pd.read_csv(file) for file in departures_files), sort=True)\n",
    "all_dep.info()\n",
    "all_dep.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dep.to_csv('data_sets/depart_concat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading multiple files with Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDep_spark =\\\n",
    "spark.read.format('csv').option('header', 'true').load(\"data_sets/AA_DFW_*.csv\", inferSchema=True)\n",
    "\n",
    "print(type(allDep_spark))\n",
    "print(allDep_spark.columns)\n",
    "print(allDep_spark.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allDep_spark.printSchema())\n",
    "print(len(allDep_spark.columns))\n",
    "print(allDep_spark.columns)\n",
    "print(allDep_spark.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDep_spark_2 = allDep_spark.withColumnRenamed('Date (MM/DD/YYYY)', 'date') \\\n",
    "                .withColumnRenamed(\"Flight Number\",\"flight\") \\\n",
    "                .withColumnRenamed(\"Destination Airport\",\"dest\")\\\n",
    "                .withColumnRenamed(\"Actual elapsed time (Minutes)\", \"delay\")\n",
    "\n",
    "allDep_spark_2.printSchema()\n",
    "print(type(allDep_spark_2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allDep_spark_2.show(3))\n",
    "print(allDep_spark_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it raises and error when date is defined as date\n",
    "# TypeError: field date: DateType can not accept object '01/01/2017' in type <class 'str'>\n",
    "# probably the conversion to date type is explicit necessary\n",
    "\n",
    "depSchema = StructType([\n",
    "\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('flight', IntegerType(), True),\n",
    "    StructField('dest', StringType(), True),\n",
    "    StructField('delay', IntegerType(), True) \n",
    "    ])   \n",
    "\n",
    "# inferSchema=True\n",
    "allDep_spark_3 = spark.createDataFrame(allDep_spark_2.rdd, schema=depSchema)\n",
    "\n",
    "# allDep_spark_3 =  allDep_spark_2.toDF()\n",
    "\n",
    "allDep_spark_3.printSchema()\n",
    "print(type(allDep_spark_3))\n",
    "print(len(allDep_spark_3.columns))\n",
    "print(allDep_spark_3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDep_spark_3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allDep_spark_3.write.parquet(\"data_sets/departures_parq.parquet\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet\n",
    "\n",
    "\n",
    "<p>\n",
    "\"Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.\"\n",
    "</p> \n",
    "<a href=\"https://parquet.apache.org/\" target=\"_blank\">Apache Parquet</a> \n",
    "\n",
    "<p>\n",
    "\"A column-oriented DBMS (or columnar database management system) is a database management system (DBMS) that stores data tables by column rather than by row. Practical use of a column store versus a row store differs little in the relational DBMS world. Both columnar and row databases can use traditional database query languages like SQL to load data and perform queries. Both row and columnar databases can become the backbone in a system to serve data for common extract, transform, load (ETL) and data visualization tools. However, by storing data in columns rather than rows, the database can more precisely access the data it needs to answer a query rather than scanning and discarding unwanted data in rows. Query performance is increased for certain workloads.\"\n",
    "</p> \n",
    "\n",
    "<a href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" target=\"_blank\">Wikipedia</a> \n",
    "\n",
    "<img src=\"columnstore.png\" alt=\"Smiley face\" height=\"600\" width=\"600\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDep_spark_3.write.parquet(\"data_sets/departures_parq.parquet\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_parquet = spark.read.parquet('data_sets/departures_parq.parquet')\n",
    "dep_parquet.createOrReplaceTempView(\"dep_parquet_table\")\n",
    "\n",
    "print(spark_session.catalog.listTables())\n",
    "dep_parquet.printSchema()\n",
    "print(dep_parquet.count())\n",
    "print(type(dep_parquet))\n",
    "print(dep_parquet.show(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL-queries on parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flight number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique flight numbers\n",
    "flight_dist = spark.sql('SELECT DISTINCT flight FROM dep_parquet_table ORDER BY flight')\n",
    "\n",
    "print(type(flight_dist))\n",
    "print(\"Unique flights: {}\".format(flight_dist.count()))\n",
    "\n",
    "flight_dist.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_flights = spark.sql('SELECT COUNT(flight) from dep_parquet_table').collect()[0]\n",
    "print('The number of flights are: %d' % count_flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by flight number and count the occurences of flights\n",
    "flight_gb = \\\n",
    "spark.sql('SELECT flight, COUNT(flight) AS count FROM dep_parquet_table GROUP BY flight ORDER BY count DESC ')\n",
    "\n",
    "flight_gb.show(10)\n",
    "print(flight_gb.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_flight = flight_gb\n",
    "print(type(count_flight))\n",
    "count_flight_pd = count_flight.toPandas()\n",
    "print(type(count_flight_pd))\n",
    "count_flight_pd.loc[:10,].plot.bar(x=\"flight\", y=\"count\", color=['khaki', 'aqua'], figsize=(15,5),\n",
    "                                   edgecolor='black', linewidth=3, \n",
    "                                   title='top ten number of flights per flight number', legend=False)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_flight_pd.loc[len(count_flight_pd)-10:len(count_flight_pd), ].\\\n",
    "plot.bar(x=\"flight\", y=\"count\", color=['lightcoral', 'cornsilk'], figsize=(15,5),\n",
    "         edgecolor='black', linewidth=3, \n",
    "         title='ten lowest number of flights per flight number', legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_delay = spark.sql('SELECT avg(delay) AS average_delay FROM dep_parquet_table')\n",
    "av_delay.show()\n",
    "\n",
    "max_delay = spark.sql('SELECT MAX(delay) AS maximum_delay FROM dep_parquet_table')\n",
    "max_delay.show()\n",
    "\n",
    "min_delay = spark.sql('SELECT MIN(delay) AS minimum_delay FROM dep_parquet_table')\n",
    "min_delay.show()\n",
    "\n",
    "sum_duration = spark.sql('SELECT sum(delay) from dep_parquet_table').collect()[0]\n",
    "print('The total delay time is: %d' % sum_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_delay = spark.sql(\"SELECT flight, AVG(delay) AS average_delay \\\n",
    "                         FROM dep_parquet_table GROUP BY flight ORDER BY average_delay DESC\")\n",
    "\n",
    "flight_delay.show(10)\n",
    "print(type(flight_delay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_delay_pd = flight_delay.toPandas()\n",
    "flight_delay_pd.loc[:10,].plot.bar(x='flight', y='average_delay', figsize=(15,5),\n",
    "                                   edgecolor='black', linewidth=3,\n",
    "                                   color=['indigo', 'lightseagreen'],\n",
    "                                   title = \"top ten flight numbers with highest average delay\",\n",
    "                                   legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_delay_pd.loc[len(flight_delay_pd)-10 : len(flight_delay_pd),].\\\n",
    "plot.bar(x='flight', y='average_delay', figsize=(15,5),\n",
    "         edgecolor='black', linewidth=3,\n",
    "         color=['indigo', 'lightseagreen'],\n",
    "         title = \"ten lowest flight numbers with average delay\",\n",
    "         legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_distinct = spark.sql('SELECT DISTINCT  dest FROM dep_parquet_table')\n",
    "\n",
    "dest_distinct.show(5)\n",
    "print(dest_distinct.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_count = spark.sql('SELECT dest, COUNT(dest) as count FROM dep_parquet_table \\\n",
    "                       GROUP BY dest ORDER BY count DESC')\n",
    "\n",
    "dest_count.show(10)\n",
    "print(dest_count.count())\n",
    "# Lax is the aiport with the most traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dest_count_pd = dest_count.toPandas()\n",
    "dest_count_pd.plot.bar(x='dest', y='count', figsize=(30,15),\n",
    "                       edgecolor=\"black\", linewidth='3', \n",
    "                       color=['lawngreen', 'crimson' , 'deepskyblue'],\n",
    "                       title=\"Count of destinations indicating traffic at aiport\",\n",
    "                       legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a columns based on conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many delays are legally enforceable for compensation by the customer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greater 50 minutes is legally enforceable by the passengers\n",
    "legal_1 = \\\n",
    "allDep_spark_2.withColumn('legally_enforceable_delay', \n",
    "                          when(allDep_spark_2.delay > 50, 'legally_enforceable_delay')\n",
    "                          .otherwise('not_legally_relevant'))\n",
    "\n",
    "print(type(legal_1))\n",
    "print(display(legal_1.show(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greater 100 minutes then compensation is legally enforceable by the passengers\n",
    "legal_2 = \\\n",
    "allDep_spark_2.withColumn('legally_enforceable', \n",
    "                          when(allDep_spark_2.delay > 100, 1)\n",
    "                          .otherwise(0))\n",
    "\n",
    "print(type(legal_2))\n",
    "print(display(legal_2.show(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_2.createOrReplaceTempView(\"legal2\")\n",
    "print(spark_session.catalog.listTables())\n",
    "# the parquet file is now registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legally_gb=spark.sql('SELECT legally_enforceable, COUNT(legally_enforceable) as count \\\n",
    "FROM legal2 GROUP BY legally_enforceable')\n",
    "legally_gb.show()\n",
    "print(type(legally_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_2_pd = legally_gb.toPandas()\n",
    "print(legal_2_pd.info())\n",
    "\n",
    "legal_2_pd.plot.bar(x='legally_enforceable', y='count', legend=False, edgecolor='red',\n",
    "                    linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(legal_2_pd.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an Id column indentifying data records\n",
    "\n",
    "<p>\n",
    "Spark offers a function to generat monotonically increasing id's.   \n",
    "This id's cannot be sequential.\n",
    "On the contrary from the outside view the id's can occur quite abritary,\n",
    "because with lazy evaluation those numbers are not actually generated till the transformations are conducted. As Spark is a distributed process the id's are createdwith regard to the partitions and not sequentially\n",
    "like you would expect from a Pandas data frame. \n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDep_spark_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDep_spark_3 = \\\n",
    "allDep_spark_2.withColumn('ID_row', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDep_spark_3.show(5)\n",
    "# Here are the id's in line with the sequential expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDep_spark_3.orderBy(allDep_spark_3.ID_row.desc()).show(5)\n",
    "# here is a jump in id row numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making distinct destinations ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_distinct = allDep_spark_2.select('dest').distinct()\n",
    "dest_distinct.show(5)\n",
    "print(dest_distinct.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_id = \\\n",
    "dest_distinct.withColumn('ID_destintations', \n",
    "                         F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_id.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_id.orderBy(dest_id.ID_destintations.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the number of partitions influence the ID-variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making df's with different number of partions\n",
    "# rdd.repartition(1)\n",
    "allDep_repart1 = allDep_spark_2.rdd.repartition(1)\n",
    "print(allDep_repart1)\n",
    "\n",
    "print(allDep_repart1.getNumPartitions())\n",
    "print(type(allDep_repart1))\n",
    "\n",
    "df_repart1 = allDep_repart1.toDF()\n",
    "print(type(df_repart1))\n",
    "print(df_repart1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making df's with different number of partions\n",
    "# rdd.repartition(20) needs more processing time than rdd.repartition(20)\n",
    "allDep_repart20 = allDep_spark_2.rdd.repartition(20)\n",
    "print(allDep_repart20)\n",
    "\n",
    "print(allDep_repart20.getNumPartitions())\n",
    "print(type(allDep_repart20))\n",
    "\n",
    "df_repart20 = allDep_repart20.toDF()\n",
    "print(type(df_repart20))\n",
    "print(df_repart20.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ID columns\n",
    "dest_id_1 = \\\n",
    "df_repart1.select('dest').distinct().\\\n",
    "withColumn('ID', F.monotonically_increasing_id())\n",
    "\n",
    "dest_id_1.orderBy(dest_id_1.ID).show(10)\n",
    "dest_id_1_max = dest_id_1.select('ID').rdd.max()[0]\n",
    "print(dest_id_1_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_id_20 = \\\n",
    "df_repart20.select('dest').distinct().\\\n",
    "withColumn('ID', F.monotonically_increasing_id())\n",
    "\n",
    "dest_id_20.orderBy(dest_id_20.ID).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_id_20_max = dest_id_20.select('ID').rdd.max()[0]\n",
    "print(dest_id_20_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handing over max ID from one df as start ID to the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(allDep_spark_2.columns)\n",
    "# create a subset df\n",
    "\n",
    "df_LAX = allDep_spark_2.filter(F.col('dest')=='LAX')\n",
    "df_LAX = df_LAX.withColumn('ID', F.monotonically_increasing_id())\n",
    "\n",
    "print(df_LAX.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine max ID\n",
    "df_LAX_max = df_LAX.select('ID').rdd.max()[0]\n",
    "print(df_LAX_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another subset\n",
    "df_ORF = allDep_spark_2.filter(F.col('dest')=='ORF')\n",
    "\n",
    "# make the max id value of LAX the start id of ORF\n",
    "df_ORF = \\\n",
    "df_ORF.withColumn('ID', F.monotonically_increasing_id()+df_LAX_max )\n",
    "# determine the max ID of this df\n",
    "df_ORF_max = df_ORF.select('ID').rdd.max()[0]\n",
    "\n",
    "print(df_ORF.count())\n",
    "print(df_ORF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_LAX_max)\n",
    "print(df_ORF_max)\n",
    "\n",
    "print(df_LAX.select('ID').show(5))\n",
    "print(df_ORF.select('ID').show(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special techniques for cleaning and improving performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "<p>\n",
    "refers in Spark to the task of storing data in memory, preferable the quick accessible Solid-state drive (SSD). This improves speed of operation as the data must not be retrieved from the nodes of the Spark cluster. Of course very large data sets may not fit into memory. This is one reason to set up a Spark cluster in the first place. Caching needs coding and the effort of coding and controlling might not be justified by performance increase every time. Monitoring of performance gains through caching is recommended. \n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Cache is a Spark transformation only executed when an action is called.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measuring performance with time\n",
    "print (\"time.time(): %f \" %  time.time())\n",
    "print (time.localtime( time.time() ))\n",
    "print (time.asctime( time.localtime(time.time()) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(allDep_spark_2))\n",
    "# print(allDep_spark_2.columns)\n",
    "# print(allDep_spark_2.show())\n",
    "\n",
    "# assigning time to start time\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows took %f seconds\" % \\\n",
    "     (allDep_spark_2.distinct().count(), time.time() - start_time))\n",
    "\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "allDep_cached = allDep_spark_2.distinct().cache()\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % \\\n",
    "      (allDep_cached.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cached version took less time than the other version.<br>\n",
    "The is_cached method allows to check if an object is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is the allDep_cached data frame cached?: %s\" % \\\n",
    "      allDep_cached.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing departures_df from cache\n",
    "# with unpersist\n",
    "allDep_cacheReversed = allDep_cached.unpersist()\n",
    "\n",
    "print(\"Is the allDep_cacheReversed data frame cached?: %s\" % \\\n",
    "      allDep_cacheReversed.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data files as full and split files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('data_sets/depart_concat.csv')\n",
    "split_df = spark.read.csv('data_sets/AA_DFW_****_Departures_Short.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_4 = time.time()\n",
    "print(\"count: %i\" % full_df.count())\n",
    "print(\"run time: %f\" % (time.time() - start_time_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_5 = time.time()\n",
    "print(\"count: %i\" % split_df.count())\n",
    "print(\"run time: %f\" % (time.time() - start_time_5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling\n",
    "\n",
    "<p>\n",
    "is a side effect of redistributing data between workers by the Spark. While executing the algorithm it will be necessary to shift data around \n",
    "to do certain tasks. It is irrelevant to know,\n",
    "which data is processed by which node, as long as it processed. \n",
    "However too much shuffling can create bottlenecks and lower data throughput time. Reducing shuffling can increase performance. Instead\n",
    "of relying on the Spark algorithm to figure out redistribution operations like broadcasting are giving nodes specified copies of data.\n",
    "</p> \n",
    "\n",
    "\n",
    "<p>\n",
    "The execution plan \"df.explain\" gives insights about how \n",
    "the Spark commands the redistribution is actually carried out and\n",
    "can determine operations like broadcasting.\n",
    "\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = \\\n",
    "spark.read.csv(path='data_sets/flights_small.csv', sep=',', \n",
    "               encoding='UTF-8', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = \\\n",
    "spark.read.csv(path='data_sets/airports.csv', sep=',', \n",
    "               encoding='UTF-8', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flights.count())\n",
    "print(len(flights.columns))\n",
    "\n",
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.select('dest').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDep_spark.printSchema()\n",
    "print(len(allDep_spark.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDep_spark.select('Destination Airport').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airFlight_join = \\\n",
    "flights.join(allDep_spark,\n",
    "             allDep_spark['Destination Airport'] == flights['dest'])\n",
    "\n",
    "print(len(airFlight_join.columns))\n",
    "print(airFlight_join.select('dest').show(5))\n",
    "airFlight_join.explain()\n",
    "# it does allready a broadcast without explicitly commanding this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airFlight_broadcast = \\\n",
    "flights.join(broadcast(allDep_spark),\n",
    "             allDep_spark['Destination Airport'] == flights['dest'])\n",
    "\n",
    "print(len(airFlight_broadcast.columns))\n",
    "airFlight_broadcast.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the time difference between with and without broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_1 = time.time()\n",
    "print(start_time)\n",
    "# do an operation whichever initiates laz evaluation\n",
    "airFlight_n = airFlight_join.count()\n",
    "duration_1 = time.time() - start_time_1\n",
    "print(duration_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_2 = time.time()\n",
    "broadcast_n = airFlight_broadcast.count()\n",
    "duration_2 = time.time() - start_time_2\n",
    "print(duration_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f\"Hello, {name}. You are {age}.\"\n",
    "print(f\"count airFlight_n: {airFlight_n}, processing time: {duration_1}\")\n",
    "print(f\"count broadcast_n: {broadcast_n}, processing time: {duration_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipelines \n",
    "\n",
    "<p>\n",
    "(in Spark) processing data from the source (e.g. csv, jsons, html, databases, web services ...) to the outputs. Between inputs and outputs the data is transformed (e.g. withColum, filter, drop, ...) and then stored as files in formats like CSV, Parquet, or a database.\n",
    "In a process of validation the data is then tested for consistency with \n",
    "expectations. Afterwards the data is ready for analysis (e.g. exploration, aggregation ...).\n",
    "</p> \n",
    "\n",
    "<p>\n",
    "Packages like Spark.ML or scikit.learn have implemented data the concept of data pipelines as special objects allowing for a structured, powerful\n",
    "workflow.\n",
    "</p> \n",
    "\n",
    "<p>\n",
    "Below there is a simple example of multistep data pipeline.\n",
    "</p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: load data\n",
    "depart17 = \\\n",
    "spark.read.format('csv').load('data_sets\\AA_DFW_2017_Departures_Short.csv',\n",
    "                              header=True)\n",
    "# header must be set to true, otherwise spark produces aliases like _c0\n",
    "\n",
    "depart17.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: rename the columns\n",
    "depart171 = \\\n",
    "depart17.withColumnRenamed('Flight Number', 'flight_n').\\\n",
    "withColumnRenamed('Date (MM/DD/YYYY)', 'date').\\\n",
    "withColumnRenamed('Destination Airport', 'dest').\\\n",
    "withColumnRenamed('Actual elapsed time (Minutes)', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart171.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart171.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: change data type of time variable\n",
    "depart172 = \\\n",
    "depart171.withColumn(\"time_elapsed\", depart171.time.cast(\"integer\")).\\\n",
    "drop('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart172.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart172.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: filter the data frame by time greater 0\n",
    "depart172_0 = depart172.filter(depart172[3] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: add an id colum to the data frame\n",
    "# this makes row operations easier of course\n",
    "depart172_id = \\\n",
    "depart172_0.withColumn('id', F.monotonically_increasing_id())\n",
    "depart172_id = depart172_id.select('id', 'date', 'flight_n', 'dest',\n",
    "                                   'time_elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart172_id.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: write it down as a json\n",
    "depart172_id.write.json('data_sets\\depart_json.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Step 6:order by\n",
    "depart172_id.orderBy('time_elapsed').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows beginning with '#'\n",
    "comment_count = \\\n",
    "depart172_id.where(col('flight_n').startswith('#')).count()\n",
    "comment_count\n",
    "# in the column flight_n there are not any comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have written some comments into the raw data\n",
    "# and reading the commented data now in \n",
    "\n",
    "commentSchema = StructType([\n",
    "    # Define the date field\n",
    "    StructField('date', StringType(), True),\n",
    "    # Add the title field\n",
    "    StructField('flight', StringType(), True),\n",
    "    # Add the voter field\n",
    "    StructField('dest', StringType(), True),\n",
    "    StructField('time', IntegerType(), True) \n",
    "    ])   \n",
    "\n",
    "commentDF = spark.read.csv('data_sets\\comments_1.csv', \n",
    "                     header=True, schema=commentSchema, quote=\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(commentDF.printSchema())\n",
    "print(commentDF.show(10, truncate = False))\n",
    "print(commentDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows beginning with '#'\n",
    "commentDF_count = \\\n",
    "commentDF.where(col('date').startswith('#')).count()\n",
    "commentDF_count\n",
    "# there are 6 comments in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withoutCommentDF = spark.read.csv('data_sets\\comments_1.csv',\n",
    "                                  comment='#',\n",
    "                                  header=True, schema=commentSchema)\n",
    "\n",
    "print(withoutCommentDF.show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any comments?\n",
    "withoutCommentDF.where(col('date').startswith('#')).count()\n",
    "# Comments are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_fields = F.split(withoutCommentDF['date'], '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the colcount column on the DataFrame\n",
    "withoutCommentDF_2 = \\\n",
    "withoutCommentDF.withColumn('colcount', F.size(tmp_fields))\n",
    "withoutCommentDF_2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining\n",
    "\n",
    "\n",
    "<p>\n",
    "\"Spark SQL supports all basic join operations available in traditional SQL, though Spark Core Joins has huge performance issues when not designed with care as it involves data shuffling across the network, In the other hand Spark SQL Joins comes with more optimization by default (thanks to DataFrames & Dataset) however still there would be some performance issues to consider while using.\"\n",
    "</p> \n",
    "<a href=\"https://sparkbyexamples.com/spark/spark-sql-dataframe-join/\" target=\"_blank\">\n",
    "Spark by {Examples}\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a primary key to join on?\n",
    "# airports.printSchema()\n",
    "# allDep_spark_2.printSchema()\n",
    "# flights.printSchema()\n",
    "# airports.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allDep_spark_2.select('dest').show(5))\n",
    "print(airports.select('faa').show(5))\n",
    "print(flights.select('origin').show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The airport abbreviations are working as a key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(airports.count())\n",
    "print(allDep_spark_2.count())\n",
    "print(flights.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(airports.columns))\n",
    "print(len(airports.columns))\n",
    "print(len(flights.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join\n",
    "\n",
    "\"Inner join is the default join in Spark and its mostly used, \n",
    "this joins two datasets on key columns and where keys dont match \n",
    "the rows get dropped from both datasets (emp & dept).\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depAirport_join = \\\n",
    "allDep_spark_2.join(F.broadcast(airports),\n",
    "                    airports.faa == allDep_spark_2.dest)\n",
    "\n",
    "print(depAirport_join.count())\n",
    "print(len(depAirport_join.columns))\n",
    "depAirport_join.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "depFlight_join = \\\n",
    "allDep_spark_2.join(F.broadcast(flights),\n",
    "                    flights.origin == allDep_spark_2.dest)\n",
    "\n",
    "print(depFlight_join.count())\n",
    "print(len(depFlight_join.columns))\n",
    "# depFlight_join.printSchema()\n",
    "# print(depFlight_join.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left, Leftouter Join\n",
    "\n",
    "Left a.k.a Leftouter join returns all rows from the left dataset regardless of match found on the right dataset when join expression doesnt match, it assigns null for that record and drops records from right where match not found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depFlight_leftjoin = \\\n",
    "allDep_spark_2.join(F.broadcast(flights),\n",
    "                    flights.origin == allDep_spark_2.dest, 'left')\n",
    "\n",
    "print(depFlight_leftjoin.count())\n",
    "print(len(depFlight_leftjoin.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depAirport_leftjoin = \\\n",
    "allDep_spark_2.join(F.broadcast(airports),\n",
    "                    airports.faa == allDep_spark_2.dest, 'left')\n",
    "\n",
    "print(depAirport_leftjoin.count())\n",
    "print(len(depAirport_leftjoin.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Anti Join\n",
    "\n",
    "\"leftanti join does the exact opposite of the leftsemi, leftanti join returns only columns from the left dataset for non-matched records.\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depFlight_antijoin = \\\n",
    "allDep_spark_2.join(F.broadcast(flights),\n",
    "                    flights.origin == allDep_spark_2.dest, 'left_anti')\n",
    "\n",
    "print(depFlight_antijoin.count())\n",
    "print(depFlight_antijoin.columns)\n",
    "# depFlight_antijoin.printSchema()\n",
    "# depFlight_antijoin.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depAirport_antijoin = \\\n",
    "allDep_spark_2.join(F.broadcast(airports),\n",
    "                    airports.faa == allDep_spark_2.dest, 'left_anti')\n",
    "\n",
    "print(depAirport_antijoin.count())\n",
    "print(len(depAirport_antijoin.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
